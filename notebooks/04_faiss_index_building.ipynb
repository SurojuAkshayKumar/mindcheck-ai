{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dcd05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c3fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 1. File paths ===\n",
    "embeddings_csv_path = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_embeddings.csv\"\n",
    "cleaned_csv_path    = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_cleaned.csv\"\n",
    "index_save_path     = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d56799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings...\n",
      "‚ö† Detected 769 columns, expected 768. Dropping the first column...\n",
      "‚úÖ Embeddings loaded with shape (149015, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Loading embeddings...\")\n",
    "df_emb = pd.read_csv(embeddings_csv_path)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "df_emb = df_emb.select_dtypes(include=[np.number])\n",
    "\n",
    "# If we still have an extra column (769 instead of 768), drop the first one\n",
    "if df_emb.shape[1] != 768:\n",
    "    print(f\"‚ö† Detected {df_emb.shape[1]} columns, expected 768. Dropping the first column...\")\n",
    "    df_emb = df_emb.iloc[:, 1:]\n",
    "\n",
    "# Ensure float32 & contiguous\n",
    "embeddings = df_emb.values.astype(\"float32\").copy()\n",
    "print(f\"‚úÖ Embeddings loaded with shape {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0d93a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading texts...\n",
      "‚úÖ Loaded 149015 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS-PC\\AppData\\Local\\Temp\\ipykernel_6396\\346236828.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_text = pd.read_csv(cleaned_csv_path)\n"
     ]
    }
   ],
   "source": [
    "# === 3. Load texts ===\n",
    "print(\"üìÇ Loading texts...\")\n",
    "df_text = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "# Try common column names for text\n",
    "possible_text_cols = [\"text\", \"body\", \"content\", \"merged_text\", \"post\"]\n",
    "text_col = None\n",
    "for col in possible_text_cols:\n",
    "    if col in df_text.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "if text_col is None:\n",
    "    text_col = df_text.columns[0]  # fallback to first column\n",
    "\n",
    "texts = df_text[text_col].astype(str).tolist()\n",
    "\n",
    "# Sanity check\n",
    "assert len(texts) == embeddings.shape[0], f\"‚ùå Mismatch: {len(texts)} texts vs {embeddings.shape[0]} embeddings\"\n",
    "print(f\"‚úÖ Loaded {len(texts)} texts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a558c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built with 149015 vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 4. Normalize & build FAISS index ===\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03699f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved FAISS index to C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\n"
     ]
    }
   ],
   "source": [
    "# === 5. Save FAISS index ===\n",
    "faiss.write_index(index, index_save_path)\n",
    "print(f\"üíæ Saved FAISS index to {index_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d20a3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Load embedding model (must match how embeddings were created) ===\n",
    "# Change to the same model name you used when creating aimind_embeddings.csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c829e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Search function with dimension check ===\n",
    "def search(query, top_k=5):\n",
    "    # Convert query to embedding\n",
    "    query_emb = model.encode([query], normalize_embeddings=True)\n",
    "    query_emb = np.array(query_emb, dtype=\"float32\")\n",
    "    \n",
    "    # Check if dimensions match\n",
    "    if query_emb.shape[1] != index.d:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch: Query embedding dim = {query_emb.shape[1]}, \"\n",
    "            f\"FAISS index dim = {index.d}. \"\n",
    "            f\"Use the same embedding model that was used to create aimind_embeddings.csv.\"\n",
    "        )\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "    # Return text and score for top_k results\n",
    "    return [(texts[i], float(scores[0][j])) for j, i in enumerate(idxs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e3b1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Results for query: 'I feel anxious and can't sleep'\n",
      "\n",
      "Score: 0.0560 | Text: Why do I go crazy if there's silence? ...\n",
      "Score: 0.0560 | Text: Not feeling valid because I don't have a special interest... ...\n",
      "Score: 0.0560 | Text: Anybody feel guilty just for existing? ...\n"
     ]
    }
   ],
   "source": [
    "# === 8. Test search ===\n",
    "sample_query = \"I feel anxious and can't sleep\"\n",
    "results = search(sample_query, top_k=3)\n",
    "\n",
    "print(f\"\\nüîç Results for query: '{sample_query}'\\n\")\n",
    "for r, s in results:\n",
    "    print(f\"Score: {s:.4f} | Text: {r[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5996772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings...\n",
      "‚ö† Detected 769 columns, expected 768. Dropping first column...\n",
      "‚úÖ Embeddings loaded with shape (149015, 768)\n",
      "üìÇ Loading texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS-PC\\AppData\\Local\\Temp\\ipykernel_6396\\2865331462.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_text = pd.read_csv(cleaned_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 149015 texts\n",
      "‚úÖ FAISS index built with 149015 vectors\n",
      "üíæ Saved FAISS index to C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\n",
      "üîç Max possible self-match score: 1.0000 (should be close to 1.0)\n",
      "\n",
      "üîç Results for query: 'I feel anxious and can't sleep'\n",
      "\n",
      "Score: 0.0560 | Text: Why do I go crazy if there's silence? ...\n",
      "Score: 0.0560 | Text: Not feeling valid because I don't have a special interest... ...\n",
      "Score: 0.0560 | Text: Anybody feel guilty just for existing? ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# === 1. File paths ===\n",
    "embeddings_csv_path = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_embeddings.csv\"\n",
    "cleaned_csv_path    = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_cleaned.csv\"\n",
    "index_save_path     = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\"\n",
    "\n",
    "# === 2. Load embedding model (EXACT same one used for CSV embeddings) ===\n",
    "# Change this to match your original generation step\n",
    "model_name = \"intfloat/multilingual-e5-base\"  \n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# === 3. Load embeddings CSV ===\n",
    "print(\"üìÇ Loading embeddings...\")\n",
    "df_emb = pd.read_csv(embeddings_csv_path)\n",
    "\n",
    "# Drop any non-numeric columns\n",
    "df_emb = df_emb.select_dtypes(include=[np.number])\n",
    "\n",
    "# If dimension is > model output size, drop first column (likely ID)\n",
    "expected_dim = model.get_sentence_embedding_dimension()\n",
    "if df_emb.shape[1] != expected_dim:\n",
    "    print(f\"‚ö† Detected {df_emb.shape[1]} columns, expected {expected_dim}. Dropping first column...\")\n",
    "    df_emb = df_emb.iloc[:, 1:]\n",
    "\n",
    "# Convert to float32 & make C-contiguous\n",
    "embeddings = df_emb.values.astype(\"float32\").copy()\n",
    "print(f\"‚úÖ Embeddings loaded with shape {embeddings.shape}\")\n",
    "\n",
    "# === 4. Load texts ===\n",
    "print(\"üìÇ Loading texts...\")\n",
    "df_text = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "# Auto-detect text column\n",
    "possible_cols = [\"text\", \"body\", \"content\", \"merged_text\", \"post\"]\n",
    "text_col = next((col for col in possible_cols if col in df_text.columns), df_text.columns[0])\n",
    "texts = df_text[text_col].astype(str).tolist()\n",
    "\n",
    "# Sanity check\n",
    "assert len(texts) == embeddings.shape[0], f\"‚ùå Mismatch: {len(texts)} texts vs {embeddings.shape[0]} embeddings\"\n",
    "print(f\"‚úÖ Loaded {len(texts)} texts\")\n",
    "\n",
    "# === 5. Normalize & build FAISS index ===\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors\")\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(index, index_save_path)\n",
    "print(f\"üíæ Saved FAISS index to {index_save_path}\")\n",
    "\n",
    "# === 6. Search function ===\n",
    "def search(query, top_k=5):\n",
    "    # Embed and normalize query\n",
    "    query_emb = model.encode([query], normalize_embeddings=True)\n",
    "    query_emb = np.array(query_emb, dtype=\"float32\")\n",
    "    \n",
    "    # Dimension check\n",
    "    if query_emb.shape[1] != index.d:\n",
    "        raise ValueError(f\"Dimension mismatch: query dim {query_emb.shape[1]}, index dim {index.d}\")\n",
    "    \n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "    return [(texts[i], float(scores[0][j])) for j, i in enumerate(idxs[0])]\n",
    "\n",
    "# === 7. Score sanity check ===\n",
    "test_vec = embeddings[0:1]\n",
    "max_score, _ = index.search(test_vec, 1)\n",
    "print(f\"üîç Max possible self-match score: {max_score[0][0]:.4f} (should be close to 1.0)\")\n",
    "\n",
    "# === 8. Test query ===\n",
    "sample_query = \"I feel anxious and can't sleep\"\n",
    "results = search(sample_query, top_k=3)\n",
    "\n",
    "print(f\"\\nüîç Results for query: '{sample_query}'\\n\")\n",
    "for r, s in results:\n",
    "    print(f\"Score: {s:.4f} | Text: {r[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b40e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 744/744 [00:00<00:00, 1.58MB/s]\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [00:00<00:00, 350kB/s]\n",
      "Downloading README.md: 3.51kB [00:00, 365kB/s]\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:00<?, ?B/s] \n",
      "Downloading (‚Ä¶)ce_transformers.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.9M/90.9M [00:06<00:00, 14.2MB/s]\n",
      "Downloading model.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.4M/90.4M [00:05<00:00, 17.1MB/s]\n",
      "Downloading model_O1.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.4M/90.4M [00:04<00:00, 20.5MB/s]\n",
      "Downloading model_O2.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.3M/90.3M [00:04<00:00, 18.6MB/s]\n",
      "Downloading model_O3.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.3M/90.3M [00:04<00:00, 19.5MB/s]\n",
      "Downloading model_O4.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45.2M/45.2M [00:02<00:00, 22.1MB/s]\n",
      "Downloading model_qint8_arm64.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0M/23.0M [00:01<00:00, 15.2MB/s]\n",
      "Downloading model_qint8_arm64.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0M/23.0M [00:00<00:00, 26.9MB/s]\n",
      "Downloading model_qint8_arm64.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0M/23.0M [00:00<00:00, 26.2MB/s]\n",
      "Downloading model_quint8_avx2.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0M/23.0M [00:01<00:00, 22.8MB/s]\n",
      "Downloading openvino_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.3M/90.3M [00:03<00:00, 23.0MB/s]\n",
      "Downloading openvino_model.xml: 211kB [00:00, ?B/s]\n",
      "Downloading (‚Ä¶)_qint8_quantized.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.9M/22.9M [00:00<00:00, 24.9MB/s]\n",
      "Downloading (‚Ä¶)_qint8_quantized.xml: 368kB [00:00, 38.3MB/s]\n",
      "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.9M/90.9M [00:03<00:00, 23.4MB/s]\n",
      "Downloading (‚Ä¶)nce_bert_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53.0/53.0 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:00<00:00, 111kB/s]\n",
      "Downloading tokenizer.json: 466kB [00:00, 26.0MB/s]\n",
      "Downloading tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:00<?, ?B/s] \n",
      "Downloading vocab.txt: 232kB [00:00, 8.60MB/s]\n",
      "Downloading modules.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a lightweight paraphrasing model\n",
    "para_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "def local_paraphrase_minilm(query, num_return_sequences=3):\n",
    "    # Simple \"paraphrasing\" by encoding and decoding nearest neighbors in embedding space\n",
    "    # Here we'll just return the same sentence as placeholder since MiniLM doesn't do generative output\n",
    "    # In a real paraphrasing setup you'd combine this with a small language model\n",
    "    return [query]  # keeping only the original for now to avoid sentencepiece issues\n",
    "\n",
    "def search_local_expanded(query, top_k=5):\n",
    "    variations = local_paraphrase_minilm(query)\n",
    "    emb_list = [model.encode([v], normalize_embeddings=True) for v in variations]\n",
    "    query_emb = np.mean(emb_list, axis=0).astype(\"float32\")\n",
    "\n",
    "    if query_emb.shape[1] != index.d:\n",
    "        raise ValueError(f\"Dim mismatch: query {query_emb.shape[1]}, index {index.d}\")\n",
    "\n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "    return [(texts[i], float(scores[0][j])) for j, i in enumerate(idxs[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a17eb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Variations for 'I feel anxious and can't sleep':\n",
      "- I feel anxious and can't sleep\n",
      "- I feel anxious and cannot sleep\n",
      "- I feel worried and can't sleep\n",
      "\n",
      "üîç Results for query: 'I feel anxious and can't sleep'\n",
      "\n",
      "Score: 0.0561 | Text: Why do I go crazy if there's silence? ...\n",
      "Score: 0.0560 | Text: will prazosin stop all dreams? ...\n",
      "Score: 0.0560 | Text: Not feeling valid because I don't have a special interest... ...\n",
      "Score: 0.0560 | Text: I feel like my trauma is never valid enough. ...\n",
      "Score: 0.0560 | Text: Crying randomly throughout the day? Does anyone find themselves doing this? I‚Äôm not sure if it‚Äôs a s...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a lightweight semantic model (no SentencePiece)\n",
    "para_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "def local_paraphrase_minilm(query, num_return_sequences=3):\n",
    "    \"\"\"Generate basic offline paraphrases without SentencePiece.\"\"\"\n",
    "    variations = [query]\n",
    "\n",
    "    # Simple variations (manual tweaks)\n",
    "    if \"can't\" in query:\n",
    "        variations.append(query.replace(\"can't\", \"cannot\"))\n",
    "    if \"I feel\" in query.lower():\n",
    "        variations.append(query.replace(\"I feel\", \"I am feeling\"))\n",
    "    if \"anxious\" in query.lower():\n",
    "        variations.append(query.replace(\"anxious\", \"worried\"))\n",
    "    if \"sleep\" in query.lower():\n",
    "        variations.append(query + \" at night\")\n",
    "\n",
    "    # Limit to unique items and return top N\n",
    "    return list(dict.fromkeys(variations))[:num_return_sequences]\n",
    "\n",
    "def search_local_expanded(query, top_k=5):\n",
    "    # Create variations\n",
    "    variations = local_paraphrase_minilm(query)\n",
    "    print(f\"\\nüîÑ Variations for '{query}':\")\n",
    "    for v in variations:\n",
    "        print(f\"- {v}\")\n",
    "\n",
    "    # Encode all variations and average embeddings\n",
    "    emb_list = [model.encode([v], normalize_embeddings=True) for v in variations]\n",
    "    query_emb = np.mean(emb_list, axis=0).astype(\"float32\")\n",
    "\n",
    "    if query_emb.shape[1] != index.d:\n",
    "        raise ValueError(f\"Dim mismatch: query {query_emb.shape[1]}, index {index.d}\")\n",
    "\n",
    "    # Search in FAISS\n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "    return [(texts[i], float(scores[0][j])) for j, i in enumerate(idxs[0])]\n",
    "\n",
    "# Test\n",
    "sample_query = \"I feel anxious and can't sleep\"\n",
    "results = search_local_expanded(sample_query, top_k=5)\n",
    "\n",
    "print(f\"\\nüîç Results for query: '{sample_query}'\\n\")\n",
    "for r, s in results:\n",
    "    print(f\"Score: {s:.4f} | Text: {r[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c04a17ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading cleaned dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS-PC\\AppData\\Local\\Temp\\ipykernel_6396\\824904758.py:14: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleaned_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 149015 documents.\n",
      "üì• Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚öô Encoding corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4657/4657 [1:12:49<00:00,  1.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings shape: (149015, 384)\n",
      "üíæ Saved embeddings to C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_embeddings.csv\n",
      "‚öô Building FAISS index...\n",
      "‚úÖ FAISS index built with 149015 vectors.\n",
      "üíæ Saved FAISS index to C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\n",
      "\n",
      "üîç Testing query: I feel anxious and can't sleep\n",
      "Score: 0.7606 | Text: Anxiety and Sleep Issues ...\n",
      "Score: 0.7606 | Text: Anxiety and Sleep Issues ...\n",
      "Score: 0.7523 | Text: Anxious but tired. ...\n",
      "Score: 0.7345 | Text: Random days of intense anxiety. Can't get out of bed ...\n",
      "Score: 0.7240 | Text: It feels like I can't feel anything when I am anxious about something ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# === 1. Paths ===\n",
    "cleaned_csv_path = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_cleaned.csv\"\n",
    "index_save_path = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\mental_health.index\"\n",
    "embeddings_save_path = r\"C:\\Users\\ASUS-PC\\Desktop\\mindcheck_ai_jupyter\\data\\aimind_embeddings.csv\"\n",
    "\n",
    "# === 2. Load cleaned text data ===\n",
    "print(\"üìÇ Loading cleaned dataset...\")\n",
    "df = pd.read_csv(cleaned_csv_path)\n",
    "# If you have text in a column like \"body\" or \"text\", change here:\n",
    "text_col = \"body\" if \"body\" in df.columns else df.columns[0]\n",
    "texts = df[text_col].astype(str).tolist()\n",
    "print(f\"‚úÖ Loaded {len(texts)} documents.\")\n",
    "\n",
    "# === 3. Load SAME embedding model for corpus and queries ===\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # or the exact model you used earlier\n",
    "print(f\"üì• Loading embedding model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# === 4. Encode corpus ===\n",
    "print(\"‚öô Encoding corpus...\")\n",
    "embeddings = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "print(f\"‚úÖ Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# === 5. Save embeddings CSV (optional for inspection) ===\n",
    "pd.DataFrame(embeddings).to_csv(embeddings_save_path, index=False)\n",
    "print(f\"üíæ Saved embeddings to {embeddings_save_path}\")\n",
    "\n",
    "# === 6. Build FAISS index ===\n",
    "print(\"‚öô Building FAISS index...\")\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # Inner product works with normalized vectors for cosine\n",
    "index.add(embeddings)\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# === 7. Save index ===\n",
    "faiss.write_index(index, index_save_path)\n",
    "print(f\"üíæ Saved FAISS index to {index_save_path}\")\n",
    "\n",
    "# === 8. Quick self-test ===\n",
    "sample_query = \"I feel anxious and can't sleep\"\n",
    "print(f\"\\nüîç Testing query: {sample_query}\")\n",
    "q_emb = model.encode([sample_query], normalize_embeddings=True).astype(\"float32\")\n",
    "scores, idxs = index.search(q_emb, k=5)\n",
    "for score, idx in zip(scores[0], idxs[0]):\n",
    "    print(f\"Score: {score:.4f} | Text: {texts[idx][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561bf54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lda_env)",
   "language": "python",
   "name": "lda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
